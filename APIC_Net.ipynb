{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output as clr\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import beta as BETA\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if(torch.cuda.is_available()):\n",
    "    deivce = \"gpu\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "plt.set_cmap(\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBP:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def rand(self, n, k_max):\n",
    "        Z = []\n",
    "        for i in range(n):\n",
    "            nu = torch.distributions.beta.Beta(self.alpha,1).sample([k_max,1])\n",
    "            p = self.break_stick_ibp(nu)\n",
    "            z = torch.distributions.bernoulli.Bernoulli(p).sample().view(1,-1)\n",
    "            Z.append(z)\n",
    "        \n",
    "        Z = torch.cat(Z, 0)\n",
    "        return Z\n",
    "    \n",
    "    def rand_nu(self, nu, n= 1):\n",
    "        p = self.break_stick_ibp(nu)\n",
    "        Z = torch.distributions.bernoulli.Bernoulli(p).sample([n])\n",
    "        return Z\n",
    "    \n",
    "    def break_stick_ibp(self, nu):\n",
    "        K_max = nu.shape[0]\n",
    "        p = []\n",
    "        p.append(nu[0,:])\n",
    "        for k in range(1,K_max):\n",
    "            p.append(p[k-1]*nu[k,:])\n",
    "        \n",
    "        p = torch.cat(p,0)\n",
    "        return p\n",
    "        \n",
    "    def break_log_stick_ibp(self, lognu):\n",
    "        K_max = nu.shape[0]\n",
    "        logp = []\n",
    "        logp.append(lognu[0,:])\n",
    "        for k in range(1,K_max):\n",
    "            logp.append(logp[k-1] + lognu[k,:])\n",
    "        \n",
    "        logp = torch.cat(logp, 0)\n",
    "        return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data#.clamp(-10,10)\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    \n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                \n",
    "                \n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * (bias_correction2.pow(0.5)) / bias_correction1\n",
    "\n",
    "                p.data += -step_size*(exp_avg.div(denom))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaRadM(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AdaRadM, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdaRadM, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data#.clamp(-10,10)\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdaRadM does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                val = (grad!=0).float()\n",
    "                state['step'] += 1# val\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "                    \n",
    "                # Projecting and Normailzing grads:\n",
    "                phi_adj = None\n",
    "                dl = None\n",
    "                if(hasattr(p, 'ang')):\n",
    "                    dl, dlm1 = p.data.shape\n",
    "                    \n",
    "                    if('ang_avg' not in state.keys()):\n",
    "                        state['ang_avg'] = torch.zeros(dl).float()\n",
    "                        state['ang_cap_avg'] = torch.zeros(dl).float()\n",
    "                        state['max_ang_avg'] = torch.tensor(0).float()\n",
    "                        state['max_ang_cap_avg'] = torch.tensor(0).float()\n",
    "\n",
    "                    phi_adj = torch.zeros_like(p.data)\n",
    "                    eps = torch.tensor(10e-6)\n",
    "                    for j in range(dl):\n",
    "                        vecw = p.data[j]\n",
    "                        magw = vecw.norm(2)+eps\n",
    "                        dirw = vecw/magw\n",
    "                        vecg = grad[j]\n",
    "                        magg = vecg.norm(2)+eps\n",
    "                        dirg = vecg/magg\n",
    "\n",
    "                        r = dirw*(torch.dot(vecg, dirw))\n",
    "                        ph = vecg - r\n",
    "#                             print((r!=r).sum())\n",
    "                        grad[j] = r\n",
    "\n",
    "                        state['ang_avg'][j].mul_(beta1).add_(1 - beta1, ph.norm(2))\n",
    "                        state['ang_cap_avg'][j].mul_(beta1).add_(1 - beta1)\n",
    "                        state['max_ang_avg'] = torch.max(state['max_ang_avg'],state['ang_avg'][j])\n",
    "                        state['max_ang_cap_avg'] = torch.max(state['max_ang_cap_avg'],state['ang_cap_avg'][j])\n",
    "\n",
    "                        phi_adj[j,:] = (state['max_ang_avg']/state['max_ang_cap_avg']).pow(0.5)\n",
    "                        phi_adj[j,:].div((state['ang_avg'][j]/state['ang_cap_avg'][j]).pow(0.5) + eps)\n",
    "                        phi_adj[j,:].mul(ph)\n",
    "                        \n",
    "                        if(state['max_ang_avg'] != state['max_ang_avg']):\n",
    "                            print(magw)\n",
    "                            assert 1 == 2\n",
    "    \n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * (bias_correction2.pow(0.5)) / bias_correction1\n",
    "\n",
    "                p.data += -step_size*(exp_avg.div(denom))\n",
    "                \n",
    "                \n",
    "                if(phi_adj is not None):\n",
    "                    for j in range(dl):\n",
    "                        vec = p.data[j,:]\n",
    "                        ph = phi_adj[j,:]\n",
    "                        dirv = -ph/(ph.norm(2) + eps)\n",
    "                        mag = vec.norm(2)\n",
    "                        ang = torch.tensor(np.cos(0.1*ph.norm(2)+eps))\n",
    "                        magc = mag/ang\n",
    "                        magb = (magc.pow(2) - mag.pow(2)).pow(0.5)\n",
    "                        vecb = dirv*magb\n",
    "                        vecc = vec + vecb\n",
    "                        fvec = vecc/(vecc.norm(2) + eps)\n",
    "                        fvec = fvec*mag\n",
    "                        p.data[j,:] = fvec\n",
    "                        \n",
    "                        if((fvec != fvec).sum() > 0):\n",
    "                            fvec[fvec!=fvec] = vec[fvec!=fvec]\n",
    "                        \n",
    "                p.data += -group['lr']*0.0002*p.data\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "BATCH_SIZE = 100\n",
    "if(os.path.isdir(os.path.join(os.getcwd(), 'data'))):\n",
    "    trainset = datasets.MNIST('./data/', train=True, download=False,\n",
    "                   transform=transforms.ToTensor())\n",
    "else:\n",
    "    trainset = datasets.MNIST('./data/', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_images(images):\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    show_image(images[0])\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stochasticIBP(nn.Module):\n",
    "    '''\n",
    "    This is a stochastic IBP layer with unlimited size of input output nodes.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias = True, alpha = 4.0, beta = 1.0, fix = [False, False]):\n",
    "        super(stochasticIBP, self).__init__()\n",
    "        '''\n",
    "        in_features : initial_input_size\n",
    "        out_features : initial_output_size\n",
    "        '''\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.beta = torch.tensor(beta)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.eps = 10e-10\n",
    "        \n",
    "        self.fix_in = fix[0]\n",
    "        self.fix_out = fix[1]\n",
    "        \n",
    "        # Lazy initialization of parameters\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features)*self.eps)\n",
    "        self.weight_std = nn.Parameter(torch.randn(self.out_features, self.in_features)*self.eps)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "            self.bias_std = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "        else:\n",
    "            self.bias = torch.randn(self.out_features, 1)*0\n",
    "            self.bias_std = torch.randn(self.out_features, 1)*0\n",
    "            \n",
    "        \n",
    "        self.beta_a = nn.Parameter(torch.zeros(self.out_features, 1) \n",
    "                                 + self.inv_softplus(self.alpha) + \n",
    "                                 torch.rand(self.out_features,1)*self.eps)\n",
    "        self.beta_b = nn.Parameter(torch.zeros(self.out_features, 1) \n",
    "                                 + self.inv_softplus(torch.tensor(self.beta)) + \n",
    "                                 torch.rand(self.out_features,1)*self.eps)\n",
    "        \n",
    "        \n",
    "        # Gumbel Bernoulli\n",
    "        self.phi = nn.Parameter(torch.randn(self.out_features,self.in_features)*self.eps)\n",
    "        self.bias_phi = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "        self.temperature = torch.zeros(self.out_features, 1) + 10\n",
    "        self.t_prior = 0.1 # prior temperature\n",
    "        \n",
    "        # RRS\n",
    "        self.rhos = torch.zeros(self.out_features + 1,1) + 0.5\n",
    "        self.curr_out = self.out_features\n",
    "        self.curr_in = self.in_features\n",
    "        \n",
    "        \n",
    "        # To use or not to use\n",
    "        self.store_KL = False\n",
    "        self.KLs = None\n",
    "        self.code = None\n",
    "        self.sample_K = True\n",
    "        \n",
    "        \n",
    "        self.weight.ang = True\n",
    "        self.weight_std.ang = True\n",
    "        self.phi.ang = True\n",
    "        \n",
    "    def inv_softplus(self, alpha):\n",
    "        with torch.no_grad():\n",
    "            mask = (alpha <= 20).float()\n",
    "            \n",
    "        ret = alpha*(1-mask) + torch.log(torch.exp(alpha) - 1)*mask\n",
    "        return ret\n",
    "        \n",
    "    def softplus(self, x):\n",
    "        with torch.no_grad():\n",
    "            mask = (x <= 20).float()\n",
    "        ret = x*(1-mask) + torch.log(torch.exp(x) + 1)*mask\n",
    "        return ret\n",
    "\n",
    "    def betaf(self, alpha, beta):\n",
    "        return alpha.lgamma().exp()/(alpha+beta).lgamma().exp()\n",
    "    \n",
    "    def forward(self, input, k = 0, sample_size = 1):\n",
    "        ################## this the layers functional part\n",
    "        if(self.code is not None):\n",
    "            return self.code[:,:self.curr_out]\n",
    "        \n",
    "        \n",
    "        x = input\n",
    "        N, D = x.shape\n",
    "        \n",
    "        if(D > self.curr_in):\n",
    "            self.update_layer(D, self.out_features)\n",
    "        \n",
    "        if(k == 0):\n",
    "            curr_K = self.curr_out\n",
    "            if(self.sample_K):\n",
    "                with torch.no_grad():\n",
    "                    rhos = list(self.rhos)[:curr_K+1]\n",
    "                    L = len(rhos)\n",
    "                    samples = []\n",
    "                    for i in range(50):\n",
    "                        k = 1\n",
    "                        while(True):\n",
    "                            u = np.random.uniform()\n",
    "                            if(u > rhos[k]):\n",
    "                                samples.append(k)\n",
    "                                break\n",
    "                            k += 1\n",
    "                            if(k > L-1):\n",
    "                                break\n",
    "\n",
    "                k = int(np.mean(samples))\n",
    "            else:\n",
    "                k = curr_K\n",
    "                \n",
    "        \n",
    "        \n",
    "        if(self.fix_out):\n",
    "            return F.linear(x, self.weight[:,:D]) + self.bias[:,:].view(1,-1)\n",
    "        \n",
    "        mu = F.linear(x, self.weight[:k,:D]) + self.bias[:k,:].view(-1,k) # N x k\n",
    "        \n",
    "        log_var = F.linear(x, self.weight_std[:k,:D]) + self.bias_std[:k,:].view(-1,k) # N x k\n",
    "        \n",
    "        ################## Gaussian reparameterization...............\n",
    "        s = torch.exp(0.5*log_var)\n",
    "        eps = torch.rand_like(s)\n",
    "        z_gauss = eps.mul(s).add_(mu)\n",
    "        \n",
    "        inter_z = F.linear(x, self.phi[:k,:D]) + self.bias_phi[:k,:].view(-1,k) # N x k\n",
    "    \n",
    "        ################## Reparameterized gumbel kumaraswamy part........\n",
    "        G1 = torch.distributions.uniform.Uniform(self.eps, \n",
    "                                1-self.eps).sample([N,k,sample_size])\n",
    "        logit_G1 = G1.log() - (1-G1).log() \n",
    "            \n",
    "        ## Sampling the Nu's with stick breaking IBP\n",
    "        a = self.softplus(self.beta_a[:k,:]).view(k,1)\n",
    "        b = self.softplus(self.beta_b[:k,:]).view(k,1)\n",
    "        U = torch.distributions.uniform.Uniform(self.eps, \n",
    "                                1-self.eps).sample([k,sample_size])\n",
    "        nu = (1-(U+self.eps).pow(1/a) + self.eps).pow(1/b).view(1,-1, sample_size)[0]\n",
    "        \n",
    "        K_max = nu.shape[0]\n",
    "        p = []\n",
    "        p.append(nu[0,:])\n",
    "        for t in range(1,K_max):\n",
    "            p.append(p[t-1]*nu[t,:])\n",
    "        p = torch.cat(p,0)\n",
    "        \n",
    "        pi = torch.distributions.bernoulli.Bernoulli(p).sample([N]).view(N,-1, sample_size)\n",
    "        logit_pi = ((pi + self.eps)/(1-pi + self.eps)).log()\n",
    "        \n",
    "        logit_alpha = logit_pi + inter_z.view(N,k,1)\n",
    "        alpha = logit_alpha.sigmoid()\n",
    "        \n",
    "        z1 = (logit_alpha + logit_G1)/self.temperature[:k,:].view(1,k,1)\n",
    "        z = z1.sigmoid()\n",
    "        \n",
    "        code = z_gauss*z.mean(dim=-1).view(N,k) # N x k   : this will the output of the network.....\n",
    "        \n",
    "        if(not self.store_KL):    \n",
    "            return code \n",
    "        \n",
    "        ################################################################################\n",
    "        \n",
    "        #        This layer code is written by \"Abhishek Kumar\" github : scakc         #\n",
    "        \n",
    "        ################################################################################\n",
    "        ####### Calculation of KL divergence for this layer with priors.....\n",
    "        curr_K = k\n",
    "        eps = self.eps\n",
    "        \n",
    "        \n",
    "        # KL Gauss\n",
    "        KL_gauss = (-0.5*(1 + log_var - mu.pow(2) - log_var.exp()).view(N,curr_K).mean(dim = 0).view(1,-1))\n",
    "        KL_gauss[KL_gauss != KL_gauss] = 0\n",
    "        KL_gauss = KL_gauss.view(curr_K, 1)# curr_K x 1\n",
    "        \n",
    "        \n",
    "        # KL Kumaraswamy \n",
    "        a = self.softplus(self.beta_a[:curr_K,:]).view(curr_K,1)\n",
    "        b = self.softplus(self.beta_b[:curr_K,:]).view(curr_K,1)\n",
    "        euler_constant = -torch.digamma(torch.tensor(1.0))\n",
    "        \n",
    "        M = 11\n",
    "        a_times_b = a*b\n",
    "        acc = 1/(1 + a_times_b)*self.betaf(1/a, b)\n",
    "        for m in range(2,M+1):\n",
    "            acc = acc + 1/(m + a_times_b)*self.betaf(m/a, b)\n",
    "\n",
    "        KL_kuma = ((a - self.alpha)/(a))*(-euler_constant -torch.digamma(b) - 1/b)\n",
    "        KL_kuma += (a.log() + b.log()) + torch.log(torch.tensor(self.betaf(self.alpha,self.beta)))\n",
    "        KL_kuma -= (b - 1)/(b)\n",
    "        KL_kuma += (self.beta-1)*b*acc\n",
    "        KL_kuma[KL_kuma != KL_kuma] = 0 \n",
    "        KL_kuma = KL_kuma.view(curr_K,1)  # curr_K x 1\n",
    "        \n",
    "        \n",
    "        # KL Gumbel \n",
    "        logit_pi = (pi+eps).log() - (1-pi+eps).log()\n",
    "        logit_x  =  (z+eps).log() - (1 -z+eps).log()\n",
    "        logit_gi = (alpha+eps).log() - (1-alpha+eps).log()\n",
    "        \n",
    "        tau = self.temperature[:curr_K,:].view(1,curr_K,1)\n",
    "        tau_prior = self.t_prior\n",
    "        \n",
    "        exp_term_p = logit_pi - logit_x*(tau)\n",
    "        exp_term_q = logit_gi - logit_x*(tau)\n",
    "        log_tau = torch.log(torch.tensor(tau, requires_grad = False))\n",
    "        log_taup = torch.log(torch.tensor(tau_prior, requires_grad = False))\n",
    "        \n",
    "        softplus = torch.nn.Softplus(threshold = 20)\n",
    "        log_pz = log_tau + exp_term_p - 2.0*softplus(exp_term_p)\n",
    "        log_qz = log_tau + exp_term_q - 2.0*softplus(exp_term_q)\n",
    "        KL_gumb = (log_qz - log_pz)\n",
    "        KL_gumb[KL_gumb != KL_gumb] = 0\n",
    "        KL_gumb = KL_gumb.mean(dim =-1).mean(dim = 0).view(curr_K,1) # curr_K x 1\n",
    "        \n",
    "        self.KLs = [KL_gauss, KL_kuma, KL_gumb]\n",
    "        self.code = code\n",
    "        \n",
    "        \n",
    "        return code\n",
    "    \n",
    "    \n",
    "    def update_layer(self, in_features, out_features):\n",
    "        \n",
    "        if(out_features > self.out_features):\n",
    "            k = out_features - self.out_features\n",
    "            with torch.no_grad():\n",
    "                self.beta_a = nn.Parameter(torch.cat((self.beta_a, torch.rand(k,1)*self.eps + self.softplus(self.alpha)), 0))\n",
    "                self.beta_b = nn.Parameter(torch.cat((self.beta_b, torch.rand(k,1)*self.eps + self.softplus(self.beta)), 0))\n",
    "\n",
    "                self.phi = nn.Parameter(torch.cat((self.phi, torch.randn(k,self.in_features)), 0)*self.eps)\n",
    "                self.weight = nn.Parameter(torch.cat((self.weight, torch.randn(k, self.in_features)), 0)*self.eps)\n",
    "                self.weight_std = nn.Parameter(torch.cat((self.weight_std, torch.randn(k, self.in_features)), 0)*self.eps)\n",
    "                self.bias = nn.Parameter(torch.cat((self.bias, torch.randn(k, 1)), 0)*self.eps)\n",
    "                self.bias_std = nn.Parameter(torch.cat((self.bias_std, torch.randn(k, 1)), 0)*self.eps)\n",
    "                self.bias_phi = nn.Parameter(torch.cat((self.bias_phi, torch.randn(k, 1)), 0)*self.eps)\n",
    "                self.rhos = torch.cat((self.rhos, torch.zeros(k,1) + 0.5), 0)\n",
    "                self.temperature = torch.cat((self.temperature, torch.zeros(k,1) + 10.0), 0)\n",
    "                \n",
    "                self.out_features = out_features\n",
    "                \n",
    "        self.curr_out = out_features\n",
    "            \n",
    "        if(in_features > self.in_features):\n",
    "            k = in_features - self.in_features\n",
    "            with torch.no_grad():\n",
    "                self.phi = nn.Parameter(torch.cat((self.phi, torch.randn(self.out_features,k)), 1)*self.eps*0)\n",
    "                self.weight = nn.Parameter(torch.cat((self.weight, torch.randn(self.out_features, k)), 1)*self.eps*0)\n",
    "                self.weight_std = nn.Parameter(torch.cat((self.weight_std, torch.randn(self.out_features, k)), 1)*self.eps*0)\n",
    "\n",
    "                self.in_features = in_features\n",
    "    \n",
    "        self.curr_in = in_features\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.weight.ang = True\n",
    "        self.weight_std.ang = True\n",
    "        self.phi.ang = True\n",
    "        \n",
    "    def constraint_proj(self):\n",
    "        with torch.no_grad():\n",
    "            self.beta_a[self.beta_a < 0.1] = 0.1\n",
    "            self.beta_b[self.beta_b < 0.1] = 0.1\n",
    "#             self.rhos[self.rhos < 10e-6] = 10e-6\n",
    "#             self.rhos[self.rhos > 1 - 10e-6] = 1 - 10e-6\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class capBatchNorm2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(capBatchNorm2d, self).__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x = input # NxD\n",
    "        eps = torch.tensor(10e-6)\n",
    "        N, D = x.shape\n",
    "        mean = x.mean(dim = 0).view(1,D)\n",
    "        std = (x-mean + eps).pow(2).sum(dim = 0).div(N).pow(0.5)\n",
    "        \n",
    "        x_norm = (x-mean)/(torch.max(std,torch.tensor(1.0)) + eps)\n",
    "        x_norm += torch.rand(x_norm.shape)*0.001\n",
    "        \n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIC(nn.Module):\n",
    "    def __init__(self, latent_variable_dim , rholr = 10e-2, lr = 0.01):\n",
    "        super(APIC, self).__init__()\n",
    "       \n",
    "        # Encoder\n",
    "        self.D = 784\n",
    "        self.h_dim = 5\n",
    "        self.sl0 = stochasticIBP(self.D,self.h_dim,True, fix = [True, False], alpha = 14.0, beta = 5.0)\n",
    "        self.sl1 = stochasticIBP(self.h_dim, latent_variable_dim)\n",
    "        self.sl2 = stochasticIBP(latent_variable_dim, self.h_dim, alpha = 14.0, beta = 5.0)\n",
    "        self.sl3 = stochasticIBP(self.h_dim, self.D,True, fix = [False, True])\n",
    "        \n",
    "        self.bn = capBatchNorm2d()\n",
    "        \n",
    "        self.sls = [self.sl0, self.sl1, self.sl2]\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.lr = lr\n",
    "        self.rholr = rholr\n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, self.D)\n",
    "        x = F.relu(self.sl0(x))\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.sl1(x))\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.sl2(x))\n",
    "        x = self.bn(x)\n",
    "        x = self.sl3(x).sigmoid()\n",
    "        return x\n",
    "    \n",
    "    def train_layer(self, layer, images, sample_max = 10):\n",
    "        # if its not a stochastic layer with fixout as False then normal flow else\n",
    "        \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        curr_K = layer.curr_out\n",
    "        layer.rhos[0] = 0.99999999999999999\n",
    "        with torch.no_grad():\n",
    "            rhos = list(layer.rhos)[:curr_K+1]\n",
    "            L = len(rhos)\n",
    "            samples = []\n",
    "            for i in range(sample_max):\n",
    "                k = 1\n",
    "                while(True):\n",
    "                    u = np.random.uniform()\n",
    "                    if(u > rhos[k]):\n",
    "                        samples.append(k)\n",
    "                        break\n",
    "                    k += 1\n",
    "                    if(k > L-1):\n",
    "                        rhos.append(0.5)\n",
    "                        L = len(rhos)\n",
    "                        \n",
    "        samples.sort()\n",
    "        new_value = int(np.max(samples[-5:]))\n",
    "        layer.update_layer(layer.in_features, new_value)\n",
    "        self.dynamize_Adam()\n",
    "        \n",
    "        \n",
    "        curr_K = layer.curr_out\n",
    "        with torch.no_grad():\n",
    "            while(len(rhos)<curr_K+1):\n",
    "                rhos.append(0.5)\n",
    "            omrho = 1 - np.array(rhos)[:curr_K+1]\n",
    "            weight = omrho*0\n",
    "            for i in range(len(samples)):\n",
    "                val = samples[i]+1\n",
    "                weight[:val] += omrho[:val]\n",
    "            weight/=len(samples)\n",
    "            weightf = torch.tensor(weight).view(1,-1).float()\n",
    "        \n",
    "        model.optimizer.zero_grad()\n",
    "        one_minus_rho = weightf[:,:curr_K+1]\n",
    "        \n",
    "        l = torch.zeros(curr_K+1,1)\n",
    "#         print(l.shape, curr_K, layer.curr_out)\n",
    "        one_minus_rho = one_minus_rho.view(-1,1)\n",
    "        weight = torch.zeros_like(one_minus_rho)\n",
    "        weight[curr_K] = one_minus_rho[curr_K]\n",
    "        for i in range(1,curr_K+1):\n",
    "            weight[curr_K-i] = weight[curr_K-i+1] + one_minus_rho[curr_K - i]\n",
    "        \n",
    "        N = images.shape[0]\n",
    "        global_multiplier = N*1000/(60000)\n",
    "        \n",
    "        \n",
    "#         print(layer.curr_out)\n",
    "        layer.store_KL = True\n",
    "        layer.code = None\n",
    "        layer.sample_K = False\n",
    "        _ = self.forward(images)\n",
    "        KL_gauss, KL_kuma, KL_gumb = layer.KLs\n",
    "#         print(KL_gauss.shape, KL_kuma.shape, KL_gumb.shape, l.shape, layer.curr_out)\n",
    "        layer.KLs = None\n",
    "        layer.store_KL = False\n",
    "        \n",
    "        l[1:,:] += KL_gauss + KL_kuma + KL_gumb\n",
    "        \n",
    "        \n",
    "        #likelihood\n",
    "        lik_loss = 0\n",
    "        eps = layer.eps\n",
    "        softplus = torch.nn.Softplus(threshold = 20)\n",
    "        for i in range(1,curr_K+1):\n",
    "            sigma2X = 0.01\n",
    "            layer.curr_out = i\n",
    "            recon_image = model(images)\n",
    "            logity = ((recon_image + eps).log() - (1 - recon_image + eps).log()).view(-1,model.D)\n",
    "            \n",
    "            Lik = images.view(-1,model.D)*logity - softplus(logity)\n",
    "            Lik = torch.sum(Lik)/(N)\n",
    "#             Lik = -F.binary_cross_entropy(recon_image, images.view(-1, model.D), reduction='sum')\n",
    "            lik_loss += Lik*one_minus_rho[i]\n",
    "            l[i,:] -= Lik\n",
    "        \n",
    "        layer.code = None\n",
    "        weight = weight[1:]\n",
    "        v0 = - lik_loss\n",
    "        v1 = (KL_gauss*weight.view(-1,1)).sum()\n",
    "        v2 = (KL_gumb*weight.view(-1,1)).sum()\n",
    "        v3 = (KL_kuma*global_multiplier*weight.view(-1,1)).sum()\n",
    "        \n",
    "        l_final_params = v0+v1+v2+v3\n",
    "        l_final_params.backward()\n",
    "        self.optimizer.step()\n",
    "        layer.constraint_proj()\n",
    "        layer.temperature /= 1.01\n",
    "        layer.temperature[layer.temperature < layer.t_prior] = layer.t_prior\n",
    "        \n",
    "        layer.sample_K = True\n",
    "        rgg = torch.zeros_like(layer.rhos)\n",
    "        for ck in samples:\n",
    "            ckp1 = ck + 1\n",
    "            rho_grads = [torch.tensor([0])]\n",
    "            ckp1 = len(rhos)\n",
    "            rho_rr = list(layer.rhos)[:ckp1]\n",
    "            l_rr = l[1:ckp1]\n",
    "\n",
    "\n",
    "            for k in range(1,ckp1):\n",
    "\n",
    "                grad = 0.0\n",
    "\n",
    "                if(k >= l.shape[0]):\n",
    "                    rho_grads.append(rho_grads[0])\n",
    "                    continue\n",
    "                else:\n",
    "                    for i in range(k-1, l_rr.shape[0]):\n",
    "                        wi = 0\n",
    "                        if(k-1 == i):\n",
    "                            wi = 1/(rho_rr[k-1] - 1 -eps)\n",
    "                        else:\n",
    "                            wi = 1/rho_rr[k-1]\n",
    "\n",
    "                        grad += (1-rho_rr[i+1])*wi*l_rr[i]\n",
    "\n",
    "                rho_grads.append(grad)\n",
    "            rho_grads = torch.tensor(rho_grads).view(-1,1)\n",
    "            rgg[:ckp1] += rho_grads\n",
    "\n",
    "        rgg/=len(samples)\n",
    "        rho_grads = rgg[:curr_K+1].clamp(-1000,1000)\n",
    "\n",
    "        rho_grads[rho_grads != rho_grads] = 0.0\n",
    "        rho_logit = ((layer.rhos).log() - (1 - layer.rhos).log())[:curr_K+1]\n",
    "        sig_rho = rho_logit.sigmoid()\n",
    "        \n",
    "\n",
    "        rho_logit[:curr_K+1,:] = rho_logit[:curr_K+1,:] - self.rholr*(sig_rho*(1-sig_rho)*rho_grads.view(-1,1))\n",
    "        with torch.no_grad():\n",
    "            layer.rhos[:curr_K+1,:] = (rho_logit).sigmoid()\n",
    "            layer.rhos[layer.rhos<0.1] = layer.rhos[layer.rhos<0.1]*0.1 + 0.1\n",
    "        return l_final_params\n",
    "        \n",
    "    \n",
    "    def dynamize_Adam(self, reset = False, amsgrad = True):\n",
    "        with torch.no_grad():\n",
    "            if(reset or self.optimizer == None):\n",
    "                self.optimizer = AdaRadM(self.parameters(), self.lr, amsgrad = amsgrad)\n",
    "                self.optimizer.step()\n",
    "            else:\n",
    "                optim = self.optimizer\n",
    "                newoptim = AdaRadM(self.parameters(), self.lr)\n",
    "\n",
    "                for i in range(len(optim.param_groups)):\n",
    "                    group_old = optim.param_groups[i]\n",
    "                    group_new = newoptim.param_groups[i]\n",
    "\n",
    "                    for j in range(len(group_old['params'])):\n",
    "                        params_old = group_old['params'][j]\n",
    "                        params_new = group_new['params'][j]\n",
    "\n",
    "                        amsgrad = group_old['amsgrad']\n",
    "                        newoptim.param_groups[i]['amsgrad'] = amsgrad\n",
    "\n",
    "\n",
    "                        state_old = optim.state[params_old]\n",
    "                        state_new = newoptim.state[params_new]\n",
    "\n",
    "                        state_new['step'] = torch.zeros_like(params_new.data)\n",
    "                        state_new['exp_avg'] = torch.zeros_like(params_new.data)\n",
    "                        state_new['exp_avg_sq'] = torch.zeros_like(params_new.data)\n",
    "                        if(hasattr(params_old, 'ang')):\n",
    "                            state_new['ang_avg'] = torch.zeros(params_new.shape[0]).float()\n",
    "                            state_new['ang_cap_avg'] = torch.zeros(params_new.shape[0]).float()\n",
    "                            state_new['max_ang_avg'] = torch.tensor(0).float()\n",
    "                            state_new['max_ang_cap_avg'] = torch.tensor(0).float()\n",
    "\n",
    "\n",
    "                        exp_avg = state_new['exp_avg']\n",
    "                        exp_avg_sq = state_new['exp_avg_sq']\n",
    "                        max_exp_avg_sq = None\n",
    "                        if(amsgrad):\n",
    "                            state_new['max_exp_avg_sq'] = torch.zeros_like(params_new.data)\n",
    "                            max_exp_avg_sq = state_new['max_exp_avg_sq']\n",
    "                            \n",
    "                        if(len(state_old) == 0):\n",
    "                            pass\n",
    "                        else:\n",
    "                            if(len(state_old['exp_avg'].shape)==2):\n",
    "                                no,do = state_old['exp_avg'].shape\n",
    "                                exp_avg[:no,:do] = state_old['exp_avg']\n",
    "                                exp_avg_sq[:no,:do] = state_old['exp_avg_sq']\n",
    "                                if(max_exp_avg_sq is not None):\n",
    "                                    max_exp_avg_sq[:no,:do] = state_old['max_exp_avg_sq']\n",
    "                                state_new['step'][:no,:do] = state_old['step']\n",
    "                                if(hasattr(params_old, 'ang')):\n",
    "                                    if('ang_avg' not in state_old.keys()):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        state_new['ang_avg'][:no] = state_old['ang_avg']\n",
    "                                        state_new['ang_cap_avg'][:no] = state_old['ang_cap_avg']\n",
    "                                        state_new['max_ang_avg'] = state_old['max_ang_avg']\n",
    "                                        state_new['max_ang_cap_avg'] = state_old['max_ang_cap_avg']\n",
    "\n",
    "                            elif(len(state_old['exp_avg'].shape)==1):\n",
    "                                no = state_old['exp_avg'].shape[0]\n",
    "                                exp_avg[:no] = state_old['exp_avg']\n",
    "                                exp_avg_sq[:no] = state_old['exp_avg_sq']\n",
    "                                if(max_exp_avg_sq is not None):\n",
    "                                    max_exp_avg_sq[:no] = state_old['max_exp_avg_sq']\n",
    "                                state_new['step'][:no] = state_old['step']\n",
    "\n",
    "                            else:\n",
    "                                assert 1 == 2 ,'error in dynamic adam'\n",
    "\n",
    "                        state_new['exp_avg'] = exp_avg\n",
    "                        state_new['exp_avg_sq'] = exp_avg_sq\n",
    "\n",
    "                        newoptim.state[params_new] = state_new\n",
    "                    \n",
    "                self.optimizer = newoptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "model = APIC(10, lr = 0.01, rholr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lr = 0.05\n",
    "model.dynamize_Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "start = 0\n",
    "L = len(model.sls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. node in  2 th layer is  8 \n",
      "layer_rhos  [1.0, 0.9980191, 0.992861, 0.9793783, 0.9461228, 0.8699294, 0.7070365, 0.3535807, 0.14756708, 0.52241546, 0.94791085, 0.9631031, 0.96036196, 0.9411824, 0.68487287]\n",
      "Epoch 1,  Loss, [1.7976198], batch 48\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    for k in range(len(model.sls)):\n",
    "        curr_layer = L -k -1\n",
    "        layer = model.sls[curr_layer]\n",
    "        counter = 1\n",
    "        for i, data in enumerate(trainloader, start):\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            l = model.train_layer(layer, images, sample_max = 50)\n",
    "            print(\"No. node in \", curr_layer, \"th layer is \", layer.curr_out, \"\\nlayer_rhos \", list(layer.rhos.view(-1).numpy()))\n",
    "            print(\"Epoch {},  Loss, {}, batch {}\".format(epoch+1, l.clone().detach().numpy()/(BATCH_SIZE*(k+1)), i))\n",
    "            clr(wait = True)\n",
    "            model.dynamize_Adam()\n",
    "            start = i\n",
    "            if(counter%100==0):\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "        \n",
    "        \n",
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(model(images).view(-1,1,28,28).detach())\n",
    "plt.show()\n",
    "show_images(images.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer_ 0 : tensor(0) tensor(0) tensor(0) tensor(0) tensor(0) tensor(0)\n",
      "Layer_ 1 : tensor(0) tensor(0) tensor(0) tensor(0) tensor(0) tensor(0)\n",
      "Layer_ 2 : tensor(0) tensor(0) tensor(0) tensor(0) tensor(0) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(model.sls):\n",
    "    print(\"Layer_\",i,':', (layer.phi!=layer.phi).sum(),(layer.bias_phi!=layer.bias_phi).sum(),(layer.weight!=layer.weight).sum()\\\n",
    "    ,(layer.bias!=layer.bias).sum(),(layer.weight_std!=layer.weight_std).sum(),(layer.bias_std!=layer.bias_std).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
