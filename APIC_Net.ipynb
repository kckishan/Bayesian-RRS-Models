{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output as clr\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import beta as BETA\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if(torch.cuda.is_available()):\n",
    "    deivce = \"gpu\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "plt.set_cmap(\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBP:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def rand(self, n, k_max):\n",
    "        Z = []\n",
    "        for i in range(n):\n",
    "            nu = torch.distributions.beta.Beta(self.alpha,1).sample([k_max,1])\n",
    "            p = self.break_stick_ibp(nu)\n",
    "            z = torch.distributions.bernoulli.Bernoulli(p).sample().view(1,-1)\n",
    "            Z.append(z)\n",
    "        \n",
    "        Z = torch.cat(Z, 0)\n",
    "        return Z\n",
    "    \n",
    "    def rand_nu(self, nu, n= 1):\n",
    "        p = self.break_stick_ibp(nu)\n",
    "        Z = torch.distributions.bernoulli.Bernoulli(p).sample([n])\n",
    "        return Z\n",
    "    \n",
    "    def break_stick_ibp(self, nu):\n",
    "        K_max = nu.shape[0]\n",
    "        p = []\n",
    "        p.append(nu[0,:])\n",
    "        for k in range(1,K_max):\n",
    "            p.append(p[k-1]*nu[k,:])\n",
    "        \n",
    "        p = torch.cat(p,0)\n",
    "        return p\n",
    "        \n",
    "    def break_log_stick_ibp(self, lognu):\n",
    "        K_max = nu.shape[0]\n",
    "        logp = []\n",
    "        logp.append(lognu[0,:])\n",
    "        for k in range(1,K_max):\n",
    "            logp.append(logp[k-1] + lognu[k,:])\n",
    "        \n",
    "        logp = torch.cat(logp, 0)\n",
    "        return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data#.clamp(-10,10)\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                val = (grad!=0).float()\n",
    "                state['step'] += 1# val\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * (bias_correction2.pow(0.5)) / bias_correction1\n",
    "\n",
    "                p.data += -step_size*(exp_avg.div(denom))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "BATCH_SIZE = 100\n",
    "if(os.path.isdir(os.path.join(os.getcwd(), 'data'))):\n",
    "    trainset = datasets.MNIST('./data/', train=True, download=False,\n",
    "                   transform=transforms.ToTensor())\n",
    "else:\n",
    "    trainset = datasets.MNIST('./data/', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_images(images):\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    show_image(images[0])\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stochasticIBP(nn.Module):\n",
    "    '''\n",
    "    This is a stochastic IBP layer with unlimited size of input output nodes.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias = True, alpha = 4.0, beta = 1.0, fix = [False, False]):\n",
    "        super(stochasticIBP, self).__init__()\n",
    "        '''\n",
    "        in_features : initial_input_size\n",
    "        out_features : initial_output_size\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.eps = 10e-10\n",
    "        \n",
    "        self.fix_in = fix[0]\n",
    "        self.fix_out = fix[1]\n",
    "        \n",
    "        # Lazy initialization of parameters\n",
    "        self.weight = nn.Parameter(torch.randn(self.in_features, self.out_features)*self.eps)\n",
    "        self.weight_std = nn.Parameter(torch.randn(self.in_features, self.out_features)*self.eps)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "            self.bias_std = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "        else:\n",
    "            self.bias = torch.randn(self.out_features, 1)*0\n",
    "            self.bias_std = torch.randn(self.out_features, 1)*0\n",
    "            \n",
    "        self.reset_parameters()\n",
    "        \n",
    "        self.beta_a = nn.Parameter(torch.zeros(self.out_features, 1) \n",
    "                                 + self.inv_softplus(self.alpha) + \n",
    "                                 torch.rand(self.out_features,1)*self.eps)\n",
    "        self.beta_b = nn.Parameter(torch.zeros(self.out_features, 1) \n",
    "                                 + self.inv_softplus(torch.tensor(self.beta)) + \n",
    "                                 torch.rand(self.out_features,1)*self.eps)\n",
    "        \n",
    "        self.euler_constant = -torch.digamma(torch.tensor(1.0))\n",
    "        \n",
    "        # Gumbel Bernoulli\n",
    "        self.phi = nn.Parameter(torch.randn((self.in_features + 1),self.out_features)*self.eps)\n",
    "        self.temperature = torch.zeros(self.out_features, 1) + 10\n",
    "        self.t_prior = 0.1 # prior temperature\n",
    "        \n",
    "        # RRS\n",
    "        self.rhos = torch.zeros(latent_variable_dim + 1,1) + 0.5\n",
    "        self.curr_out = self.out_features\n",
    "        \n",
    "        \n",
    "\n",
    "    def inv_softplus(self, alpha):\n",
    "        with torch.no_grad():\n",
    "            mask = (alpha <= 20).float()\n",
    "            \n",
    "        ret = alpha*(1-mask) + torch.log(torch.exp(alpha) - 1)*mask\n",
    "        return ret\n",
    "        \n",
    "    def softplus(self, x):\n",
    "        with torch.no_grad():\n",
    "            mask = (x <= 20).float()\n",
    "        ret = x*(1-mask) + torch.log(torch.exp(x) + 1)*mask\n",
    "        return ret\n",
    "    \n",
    "    def forward(self, input, k = 0, sample_size = 1, store_KL = False):\n",
    "        ################## this the layers functional part\n",
    "        x = input.view(-1, self.in_features)\n",
    "        N, D = x.shape\n",
    "        \n",
    "        if(k == 0):\n",
    "            k = self.curr_out\n",
    "        \n",
    "        mu = F.linear(x, self.weight[:k,:]) + self.bias[:k,:].view(-1,k) # N x k\n",
    "        log_var = F.linear(x, self.weight_std[:k,:]) + self.bias_std[:k,:].view(-1,k) # N x k\n",
    "        \n",
    "        ################## Gaussian reparameterization...............\n",
    "        s = torch.exp(0.5*log_var)\n",
    "        eps = torch.rand_like(s)\n",
    "        a = eps.mul(s).add_(mu)\n",
    "        \n",
    "        x_cat = torch.cat((x, torch.ones(N).view(N,-1)), 1).view(N,D+1) # N x D+1 \n",
    "        inter_z = torch.mm(x_cat, self.phi[:,:k]) # N x k\n",
    "    \n",
    "        ################## Reparameterized gumbel kumaraswamy part........\n",
    "        G1 = torch.distributions.uniform.Uniform(self.eps, \n",
    "                                1-self.eps).sample([N,k,sample_size])\n",
    "        logit_G1 = G1.log() - (1-G1).log() \n",
    "            \n",
    "        ## Sampling the Nu's with stick breaking IBP\n",
    "        a = self.softplus(self.beta_a[:k,:]).view(k,1)\n",
    "        b = self.softplus(self.beta_b[:k,:]).view(k,1)\n",
    "        U = torch.distributions.uniform.Uniform(self.eps, \n",
    "                                1-self.eps).sample([k,sample_size])\n",
    "        nu = (1-(U+self.eps).pow(1/a) + self.eps).pow(1/b).view(1,-1, sample_size)[0]\n",
    "        \n",
    "        K_max = nu.shape[0]\n",
    "        p = []\n",
    "        p.append(nu[0,:])\n",
    "        for t in range(1,K_max):\n",
    "            p.append(p[t-1]*nu[t,:])\n",
    "        p = torch.cat(p,0)\n",
    "        \n",
    "        pi = orch.distributions.bernoulli.Bernoulli(p).sample([N])\n",
    "        logit_pi = ((pi + self.eps)/(1-pi + self.eps)).log()\n",
    "        logit_alpha = logit_pi + inter_z.view(N,k,1)\n",
    "        alpha = logit_alpha.sigmoid()\n",
    "        \n",
    "        z1 = (logit_alpha + logit_G1)/self.temperature[:K,:].view(1,K,1)\n",
    "        z = z1.sigmoid()\n",
    "        \n",
    "        code = a*z # N x k   : this will the output of the network.....\n",
    "        \n",
    "        if(not store_KL):    \n",
    "            return code \n",
    "        \n",
    "#         '''The forward pass of model is complete we just need KL divergences now...'''\n",
    "#         ################### Calculation of KL divergence for this layer with priors.....\n",
    "#         curr_K = self.curr_out\n",
    "        \n",
    "        \n",
    "        # gi : alpha, py : nu\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIC(nn.Module):\n",
    "    def __init__(self, latent_variable_dim, prior , rholr = 10e-12, lr = 0.01):\n",
    "        super(APIC, self).__init__()\n",
    "        \n",
    "        alpha = prior.alpha\n",
    "        self.prior = prior\n",
    "        self.print = False\n",
    "        self.sp = torch.nn.Softplus(threshold = 20)\n",
    "        \n",
    "        ### Global Params\n",
    "        self.eps1 = torch.tensor(10e-6).float()\n",
    "        self.eps2 = torch.tensor(10e-4).float()\n",
    "        self.lr = lr\n",
    "        self.D = 784\n",
    "    \n",
    "        # Encoder\n",
    "        self.h_dim = 400\n",
    "        self.fc1 = nn.Linear(self.D,self.h_dim,'relu')\n",
    "        \n",
    "        self.weight_enc_mean = nn.Parameter(torch.randn(latent_variable_dim, self.h_dim)*0.0001*self.eps1)\n",
    "        self.weight_enc_std = nn.Parameter(torch.randn(latent_variable_dim, self.h_dim)*0.0001*self.eps1)\n",
    "        self.bias_enc_mean = nn.Parameter(torch.randn(latent_variable_dim, 1)*0.0001*self.eps1)\n",
    "        self.bias_enc_std = nn.Parameter(torch.randn(latent_variable_dim, 1)*0.0001*self.eps1)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        self.weight_dec_code = nn.Parameter(torch.randn(self.h_dim, latent_variable_dim, )*0.0001*self.eps1)\n",
    "        self.bias_dec_code = nn.Parameter(torch.randn(self.h_dim, 1)*0.0001*self.eps1)\n",
    "        self.fc2 = nn.Linear(self.h_dim,self.D,'linear')\n",
    "        \n",
    "        \n",
    "        # structured SBC\n",
    "        self.aeys = nn.Parameter(torch.zeros(latent_variable_dim, 1) + self.inv_softplus(alpha) + torch.rand(latent_variable_dim,1)*0.0001*self.eps1)\n",
    "        self.bees = nn.Parameter(torch.zeros(latent_variable_dim, 1) + self.inv_softplus(torch.tensor(1.0)) + torch.rand(latent_variable_dim,1)*0.0001*self.eps1)\n",
    "        self.alpha = alpha\n",
    "        self.beta = 1.0\n",
    "        \n",
    "        \n",
    "        self.unif_sampler = torch.distributions.uniform.Uniform(self.eps1, 1-self.eps1)\n",
    "        self.euler_constant = -torch.digamma(torch.tensor(1.0))\n",
    "        \n",
    "        # Z : GumbelBernoulli\n",
    "        self.phi = nn.Parameter(torch.randn((self.h_dim + 1),latent_variable_dim)*0.0001*self.eps1)\n",
    "        self.temperature = torch.zeros(latent_variable_dim, 1) + 10\n",
    "        self.t_prior = 0.1 # prior lambda\n",
    "\n",
    "        ### Russian Roulette part\n",
    "        self.rhos = torch.zeros(latent_variable_dim + 1,1) + 0.5\n",
    "        self.rholr = rholr\n",
    "        \n",
    "        ## Optimizer\n",
    "        self.optimizer = None\n",
    "        self.K = latent_variable_dim\n",
    "        self.max_K = latent_variable_dim\n",
    "        \n",
    "        \n",
    "        \n",
    "                             \n",
    "    def inv_softplus(self, alpha):\n",
    "#         return self.sp(alpha)\n",
    "        with torch.no_grad():\n",
    "            mask = (alpha <= 20).float()\n",
    "            \n",
    "        ret = alpha*(1-mask) + torch.log(torch.exp(alpha) - 1)*mask\n",
    "        return ret\n",
    "        \n",
    "    def softplus(self, x):\n",
    "#         return torch.log(torch.exp(x) + 1)\n",
    "        with torch.no_grad():\n",
    "            mask = (x <= 20).float()\n",
    "        ret = x*(1-mask) + torch.log(torch.exp(x) + 1)*mask\n",
    "        return ret\n",
    "    \n",
    "    def sample_nupi(self, N, K, sample_size = 1):\n",
    "        \n",
    "        a = self.softplus(self.aeys)[:K,:].view(K,1)\n",
    "        b = self.softplus(self.bees)[:K,:].view(K,1)\n",
    "        \n",
    "        U = self.unif_sampler.sample([K,sample_size])\n",
    "        nu = (1-(U+self.eps1).pow(1/a) + self.eps1 ).pow(1/b).view(1,-1, sample_size)[0]\n",
    "        py = self.prior.rand_nu(nu, N).view(N,-1, sample_size)\n",
    "        \n",
    "        return nu, py\n",
    "        \n",
    "    \n",
    "    def GumbelBernoulliLogitLazyDense(self, x, k):\n",
    "        \n",
    "        N, D = x.shape\n",
    "        \n",
    "        x_cat = torch.cat((x.view(N,self.h_dim), torch.ones(N).view(N,-1)), 1).view(N,D+1)\n",
    "        inter_z = torch.mm(x_cat, self.phi[:,:k])\n",
    "        z, gi, pi, py = self.reparameterize_gumbel_kumaraswamy(inter_z, k) # N x K\n",
    "        return z, gi, pi, py\n",
    "        \n",
    "    def reparameterize_gaussian(self, log_var, mu):\n",
    "        s = torch.exp(0.5*log_var)\n",
    "        eps = torch.rand_like(s) # generate a iid standard normal same shape as s\n",
    "        return eps.mul(s).add_(mu)\n",
    "    \n",
    "    def reparameterize_gumbel_kumaraswamy(self, inter_z, p):\n",
    "        '''Proper Sampling is required with masking'''\n",
    "        \n",
    "        N, K = inter_z.shape\n",
    "        sample_size = 1\n",
    "        \n",
    "        G1 = self.unif_sampler.sample([N,K,sample_size])\n",
    "        logit_G1 = G1.log() - (1-G1).log() \n",
    "        \n",
    "        nu, pi = self.sample_nupi(N, K,sample_size) # K x sample_size\n",
    "        \n",
    "        logit_pi = ((pi + self.eps1)/(1-pi + self.eps1)).log()\n",
    "        logit_alpha = logit_pi + inter_z.view(N,K,1)\n",
    "        alpha = logit_alpha.sigmoid()\n",
    "        \n",
    "        z1 = (logit_alpha + logit_G1)/self.temperature[:K,:].view(1,K,1)\n",
    "        y = z1.sigmoid()\n",
    "        \n",
    "        return y, alpha, pi, nu\n",
    "        \n",
    "    def forward(self, input, k):\n",
    "        x = input.view(-1, self.D)\n",
    "        N, D = x.shape\n",
    "        \n",
    "        if(k == 0):\n",
    "            k = self.get_current_K()\n",
    "            \n",
    "        z, gi, pi, py, a, mu, log_var = self.encode(x,k)\n",
    "        x = self.decode(a, z.mean(dim=-1).view(N,k), k)\n",
    "        return x, z, gi, pi, py, a, mu, log_var\n",
    "    \n",
    "    \n",
    "    def encode(self, x, k):\n",
    "        \n",
    "        h = self.fc1(x)\n",
    "        log_s = ((F.linear(h, self.weight_enc_std[:k,:]) + self.bias_enc_std[:k,:].view(-1,k))/100)#.clamp(0,100)\n",
    "        mu = F.linear(h, self.weight_enc_mean[:k,:]) + self.bias_enc_mean[:k,:].view(-1,k)\n",
    "        \n",
    "        a = self.reparameterize_gaussian(log_s, mu) # N x K\n",
    "        \n",
    "        z, gi, pi, py = self.GumbelBernoulliLogitLazyDense(h, k)\n",
    "        return z,gi,pi,py,a,mu,log_s\n",
    "\n",
    "    \n",
    "    def decode(self, a, z, k):\n",
    "        \n",
    "        code = a*z\n",
    "        x = F.linear(code, self.weight_dec_code[:,:k]) + self.bias_dec_code.view(-1,self.h_dim)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x).sigmoid()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def add_k_node(self, k):\n",
    "        \n",
    "        # Add k latent features ...\n",
    "        if(k == 0):\n",
    "            return \n",
    "        with torch.no_grad():\n",
    "            self.aeys = nn.Parameter(torch.cat((self.aeys, torch.rand(k,1)*0.0001*self.eps1 + self.softplus(self.alpha)), 0))\n",
    "            self.bees = nn.Parameter(torch.cat((self.bees, torch.rand(k,1)*0.0001*self.eps1 + self.softplus(torch.tensor(1.0))), 0))\n",
    "            \n",
    "            self.phi = nn.Parameter(torch.cat((self.phi, torch.randn((self.h_dim + 1),k)), 1))\n",
    "            self.weight_enc_mean = nn.Parameter(torch.cat((self.weight_enc_mean, torch.randn(k, self.h_dim)), 0)*0.0001*self.eps1)\n",
    "            self.weight_enc_std = nn.Parameter(torch.cat((self.weight_enc_std, torch.randn(k, self.h_dim)), 0)*0.0001*self.eps1)\n",
    "            \n",
    "            self.bias_enc_mean = nn.Parameter(torch.cat((self.bias_enc_mean, torch.randn(k, 1)), 0)*0.0001*self.eps1)\n",
    "            self.bias_enc_std = nn.Parameter(torch.cat((self.bias_enc_std, torch.randn(k, 1)), 0)*0.0001*self.eps1)\n",
    "            \n",
    "            self.weight_dec_code = nn.Parameter(torch.cat((self.weight_dec_code, torch.randn(self.h_dim,k)), 1)*0.0001*self.eps1)\n",
    "            \n",
    "            self.rhos = torch.cat((self.rhos, torch.zeros(k,1) + 0.5), 0)\n",
    "            self.temperature = torch.cat((self.temperature, torch.zeros(k,1) + 10.0), 0)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def del_k_node(self, k):\n",
    "        \n",
    "        # Retain k Latent Features ...\n",
    "        if(k == 0 or k == self.get_current_K()):\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            c_K = self.get_current_K()\n",
    "            \n",
    "            self.aeys = nn.Parameter(list(torch.split(self.aeys, c_K - k , 0))[0])\n",
    "            self.bees = nn.Parameter(list(torch.split(self.bees, c_K - k , 0))[0])\n",
    "            \n",
    "            self.phi = nn.Parameter(list(torch.split(self.phi, c_K - k , 1))[0])\n",
    "            self.weight_enc_mean = nn.Parameter(list(torch.split(self.weight_enc_mean, c_K - k , 0))[0])\n",
    "            self.weight_enc_std = nn.Parameter(list(torch.split(self.weight_enc_std, c_K - k , 0))[0])\n",
    "            self.weight_dec_code = nn.Parameter(list(torch.split(self.weight_dec_code, c_K - k , 1))[0])\n",
    "            \n",
    "            self.rhos = list(torch.split(self.rhos, c_K - k + 1, 0))[0]\n",
    "                             \n",
    "    def get_current_K(self):\n",
    "        return self.K\n",
    "    \n",
    "    def constraint_proj(self):\n",
    "        with torch.no_grad():\n",
    "            self.aeys[self.aeys < 0.1] = 0.1\n",
    "            self.bees[self.bees < 0.1] = 0.1\n",
    "            self.rhos[self.rhos < 10e-6] = 10e-6\n",
    "            self.rhos[self.rhos > 1 - 10e-6] = 1 - 10e-6\n",
    "            pass\n",
    "        \n",
    "    def dynamize_Adam(self, reset = False, amsgrad = True):\n",
    "        with torch.no_grad():\n",
    "            if(reset or self.optimizer == None):\n",
    "                self.optimizer = Adam(self.parameters(), self.lr, amsgrad = amsgrad)\n",
    "                self.optimizer.step()\n",
    "            else:\n",
    "                optim = self.optimizer\n",
    "                newoptim = Adam(self.parameters(), self.lr)\n",
    "\n",
    "                for i in range(len(optim.param_groups)):\n",
    "                    group_old = optim.param_groups[i]\n",
    "                    group_new = newoptim.param_groups[i]\n",
    "\n",
    "                    for j in range(len(group_old['params'])):\n",
    "                        params_old = group_old['params'][j]\n",
    "                        params_new = group_new['params'][j]\n",
    "\n",
    "                        amsgrad = group_old['amsgrad']\n",
    "                        newoptim.param_groups[i]['amsgrad'] = amsgrad\n",
    "\n",
    "\n",
    "                        state_old = optim.state[params_old]\n",
    "                        state_new = newoptim.state[params_new]\n",
    "\n",
    "                        state_new['step'] = torch.zeros_like(params_new.data)\n",
    "\n",
    "                        state_new['exp_avg'] = torch.zeros_like(params_new.data)\n",
    "                        state_new['exp_avg_sq'] = torch.zeros_like(params_new.data)\n",
    "\n",
    "\n",
    "\n",
    "                        exp_avg = state_new['exp_avg']\n",
    "                        exp_avg_sq = state_new['exp_avg_sq']\n",
    "                        max_exp_avg_sq = None\n",
    "                        if(amsgrad):\n",
    "                            state_new['max_exp_avg_sq'] = torch.zeros_like(params_new.data)\n",
    "                            max_exp_avg_sq = state_new['max_exp_avg_sq']\n",
    "                            \n",
    "                        if(len(state_old) == 0):\n",
    "                            pass\n",
    "                        else:\n",
    "                            if(len(state_old['exp_avg'].shape)==2):\n",
    "                                no,do = state_old['exp_avg'].shape\n",
    "                                exp_avg[:no,:do] = state_old['exp_avg']\n",
    "                                exp_avg_sq[:no,:do] = state_old['exp_avg_sq']\n",
    "                                if(max_exp_avg_sq is not None):\n",
    "                                    max_exp_avg_sq[:no,:do] = state_old['max_exp_avg_sq']\n",
    "                                state_new['step'][:no,:do] = state_old['step']\n",
    "\n",
    "                            elif(len(state_old['exp_avg'].shape)==1):\n",
    "                                no = state_old['exp_avg'].shape[0]\n",
    "                                exp_avg[:no] = state_old['exp_avg']\n",
    "                                exp_avg_sq[:no] = state_old['exp_avg_sq']\n",
    "                                if(max_exp_avg_sq is not None):\n",
    "                                    max_exp_avg_sq[:no] = state_old['max_exp_avg_sq']\n",
    "                                state_new['step'][:no] = state_old['step']\n",
    "\n",
    "                            else:\n",
    "                                assert 1 == 2 ,'error in dynamic adam'\n",
    "\n",
    "                        state_new['exp_avg'] = exp_avg\n",
    "                        state_new['exp_avg_sq'] = exp_avg_sq\n",
    "\n",
    "                        newoptim.state[params_new] = state_new\n",
    "                    \n",
    "                self.optimizer = newoptim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
