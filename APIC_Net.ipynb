{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output as clr\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import beta as BETA\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if(torch.cuda.is_available()):\n",
    "    deivce = \"gpu\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "plt.set_cmap(\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBP:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def rand(self, n, k_max):\n",
    "        Z = []\n",
    "        for i in range(n):\n",
    "            nu = torch.distributions.beta.Beta(self.alpha,1).sample([k_max,1])\n",
    "            p = self.break_stick_ibp(nu)\n",
    "            z = torch.distributions.bernoulli.Bernoulli(p).sample().view(1,-1)\n",
    "            Z.append(z)\n",
    "        \n",
    "        Z = torch.cat(Z, 0)\n",
    "        return Z\n",
    "    \n",
    "    def rand_nu(self, nu, n= 1):\n",
    "        p = self.break_stick_ibp(nu)\n",
    "        Z = torch.distributions.bernoulli.Bernoulli(p).sample([n])\n",
    "        return Z\n",
    "    \n",
    "    def break_stick_ibp(self, nu):\n",
    "        K_max = nu.shape[0]\n",
    "        p = []\n",
    "        p.append(nu[0,:])\n",
    "        for k in range(1,K_max):\n",
    "            p.append(p[k-1]*nu[k,:])\n",
    "        \n",
    "        p = torch.cat(p,0)\n",
    "        return p\n",
    "        \n",
    "    def break_log_stick_ibp(self, lognu):\n",
    "        K_max = nu.shape[0]\n",
    "        logp = []\n",
    "        logp.append(lognu[0,:])\n",
    "        for k in range(1,K_max):\n",
    "            logp.append(logp[k-1] + lognu[k,:])\n",
    "        \n",
    "        logp = torch.cat(logp, 0)\n",
    "        return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data#.clamp(-10,10)\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    \n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                \n",
    "                \n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * (bias_correction2.pow(0.5)) / bias_correction1\n",
    "\n",
    "                p.data += -step_size*(exp_avg.div(denom))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaRadM(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AdaRadM, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdaRadM, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data#.clamp(-10,10)\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdaRadM does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                val = (grad!=0).float()\n",
    "                state['step'] += 1# val\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "                    \n",
    "                # Projecting and Normailzing grads:\n",
    "                phi_adj = None\n",
    "                dl = None\n",
    "                if(len(p.data.shape) == 2):\n",
    "                    dl, dlm1 = p.data.shape\n",
    "                    \n",
    "                    if('ang_avg' not in state.keys()):\n",
    "                        state['ang_avg'] = torch.zeros(dl).float()\n",
    "                        state['ang_cap_avg'] = torch.zeros(dl).float()\n",
    "                        state['max_ang_avg'] = torch.tensor(0).float()\n",
    "                        state['max_ang_cap_avg'] = torch.tensor(0).float()\n",
    "                    \n",
    "                    phi_adj = torch.zeros_like(p.data)\n",
    "                    eps = torch.tensor(10e-6)\n",
    "                    for j in range(dl):\n",
    "                        vecw = p.data[j]\n",
    "                        magw = vecw.norm(2)\n",
    "                        dirw = vecw/magw\n",
    "                        vecg = grad[j]\n",
    "                        magg = vecg.norm(2)\n",
    "                        dirg = vecg/magg\n",
    "                        \n",
    "                        r = dirw*(torch.dot(vecg, dirw))\n",
    "                        ph = vecg - r\n",
    "                        \n",
    "                        grad[j] = r\n",
    "                    \n",
    "                        state['ang_avg'][j].mul_(beta1).add_(1 - beta1, ph.norm(2))\n",
    "                        state['ang_cap_avg'][j].mul_(beta1).add_(1 - beta1)\n",
    "                        state['max_ang_avg'] = torch.max(state['max_ang_avg'],state['ang_avg'][j])\n",
    "                        state['max_ang_cap_avg'] = torch.max(state['max_ang_cap_avg'],state['ang_cap_avg'][j])\n",
    "                        \n",
    "                        phi_adj[j,:] = (state['max_ang_avg']/state['max_ang_cap_avg']).pow(0.5)\n",
    "                        phi_adj[j,:].div((state['ang_avg'][j]/state['ang_cap_avg'][j]).pow(0.5) + eps)\n",
    "                        phi_adj[j,:].mul(ph)\n",
    "    \n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * (bias_correction2.pow(0.5)) / bias_correction1\n",
    "\n",
    "                p.data += -step_size*(exp_avg.div(denom))\n",
    "                \n",
    "                \n",
    "                if(phi_adj is not None):\n",
    "                    for j in range(dl):\n",
    "                        vec = p.data[j,:]\n",
    "                        ph = phi_adj[j,:]\n",
    "                        dirv = -ph.div(ph.norm(2))\n",
    "                        mag = vec.norm(2)\n",
    "                        ang = torch.tensor(np.cos(ph.norm(2)+eps))\n",
    "                        print(\"Angle\",ang )\n",
    "                        \n",
    "                        magc = mag/ang\n",
    "                        magb = (magc.pow(2) - mag.pow(2)).pow(0.5)+eps\n",
    "                        vecb = dirv*magb\n",
    "                        vecc = vec + vecb\n",
    "                        fvec = vecc/vecc.norm(2)\n",
    "                        fvec = fvec*mag\n",
    "                        p.data[j,:] = fvec\n",
    "                        \n",
    "                p.data += -group['lr']*0.002*p.data\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "BATCH_SIZE = 100\n",
    "if(os.path.isdir(os.path.join(os.getcwd(), 'data'))):\n",
    "    trainset = datasets.MNIST('./data/', train=True, download=False,\n",
    "                   transform=transforms.ToTensor())\n",
    "else:\n",
    "    trainset = datasets.MNIST('./data/', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_images(images):\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    show_image(images[0])\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stochasticIBP(nn.Module):\n",
    "    '''\n",
    "    This is a stochastic IBP layer with unlimited size of input output nodes.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias = True, alpha = 4.0, beta = 1.0, fix = [False, False]):\n",
    "        super(stochasticIBP, self).__init__()\n",
    "        '''\n",
    "        in_features : initial_input_size\n",
    "        out_features : initial_output_size\n",
    "        '''\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.beta = torch.tensor(beta)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.eps = 10e-10\n",
    "        \n",
    "        self.fix_in = fix[0]\n",
    "        self.fix_out = fix[1]\n",
    "        \n",
    "        # Lazy initialization of parameters\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features)*self.eps)\n",
    "        self.weight_std = nn.Parameter(torch.randn(self.out_features, self.in_features)*self.eps)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "            self.bias_std = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "        else:\n",
    "            self.bias = torch.randn(self.out_features, 1)*0\n",
    "            self.bias_std = torch.randn(self.out_features, 1)*0\n",
    "            \n",
    "        \n",
    "        self.beta_a = nn.Parameter(torch.zeros(self.out_features, 1) \n",
    "                                 + self.inv_softplus(self.alpha) + \n",
    "                                 torch.rand(self.out_features,1)*self.eps)\n",
    "        self.beta_b = nn.Parameter(torch.zeros(self.out_features, 1) \n",
    "                                 + self.inv_softplus(torch.tensor(self.beta)) + \n",
    "                                 torch.rand(self.out_features,1)*self.eps)\n",
    "        \n",
    "        \n",
    "        # Gumbel Bernoulli\n",
    "        self.phi = nn.Parameter(torch.randn(self.in_features,self.out_features)*self.eps)\n",
    "        self.bias_phi = nn.Parameter(torch.randn(self.out_features, 1)*self.eps)\n",
    "        self.temperature = torch.zeros(self.out_features, 1) + 10\n",
    "        self.t_prior = 0.1 # prior temperature\n",
    "        \n",
    "        # RRS\n",
    "        self.rhos = torch.zeros(self.out_features + 1,1) + 0.5\n",
    "        self.curr_out = self.out_features\n",
    "        self.curr_in = self.in_features\n",
    "        \n",
    "        \n",
    "        # To use or not to use\n",
    "        self.store_KL = False\n",
    "        self.KLs = None\n",
    "        self.code = None\n",
    "        \n",
    "    def inv_softplus(self, alpha):\n",
    "        with torch.no_grad():\n",
    "            mask = (alpha <= 20).float()\n",
    "            \n",
    "        ret = alpha*(1-mask) + torch.log(torch.exp(alpha) - 1)*mask\n",
    "        return ret\n",
    "        \n",
    "    def softplus(self, x):\n",
    "        with torch.no_grad():\n",
    "            mask = (x <= 20).float()\n",
    "        ret = x*(1-mask) + torch.log(torch.exp(x) + 1)*mask\n",
    "        return ret\n",
    "    \n",
    "    def forward(self, input, k = 0, sample_size = 1):\n",
    "        ################## this the layers functional part\n",
    "        if(self.code is not None):\n",
    "            return self.code[:,:self.curr_out]\n",
    "        \n",
    "        \n",
    "        x = input\n",
    "        N, D = x.shape\n",
    "        \n",
    "        if(D > self.curr_in):\n",
    "            self.update_layer(D, self.out_features)\n",
    "        \n",
    "        if(k == 0):\n",
    "            k = self.curr_out\n",
    "        \n",
    "        \n",
    "        mu = F.linear(x, self.weight[:k,:D]) + self.bias[:k,:].view(-1,k) # N x k\n",
    "        if(self.fix_out):\n",
    "            return mu\n",
    "        \n",
    "        log_var = F.linear(x, self.weight_std[:k,:D]) + self.bias_std[:k,:].view(-1,k) # N x k\n",
    "        \n",
    "        ################## Gaussian reparameterization...............\n",
    "        s = torch.exp(0.5*log_var)\n",
    "        eps = torch.rand_like(s)\n",
    "        z_gauss = eps.mul(s).add_(mu)\n",
    "        \n",
    "        inter_z = torch.mm(x, self.phi[:D,:k]) + self.bias_phi[:k,:].view(-1,k) # N x k\n",
    "    \n",
    "        ################## Reparameterized gumbel kumaraswamy part........\n",
    "        G1 = torch.distributions.uniform.Uniform(self.eps, \n",
    "                                1-self.eps).sample([N,k,sample_size])\n",
    "        logit_G1 = G1.log() - (1-G1).log() \n",
    "            \n",
    "        ## Sampling the Nu's with stick breaking IBP\n",
    "        a = self.softplus(self.beta_a[:k,:]).view(k,1)\n",
    "        b = self.softplus(self.beta_b[:k,:]).view(k,1)\n",
    "        U = torch.distributions.uniform.Uniform(self.eps, \n",
    "                                1-self.eps).sample([k,sample_size])\n",
    "        nu = (1-(U+self.eps).pow(1/a) + self.eps).pow(1/b).view(1,-1, sample_size)[0]\n",
    "        \n",
    "        K_max = nu.shape[0]\n",
    "        p = []\n",
    "        p.append(nu[0,:])\n",
    "        for t in range(1,K_max):\n",
    "            p.append(p[t-1]*nu[t,:])\n",
    "        p = torch.cat(p,0)\n",
    "        \n",
    "        pi = torch.distributions.bernoulli.Bernoulli(p).sample([N]).view(N,-1, sample_size)\n",
    "        logit_pi = ((pi + self.eps)/(1-pi + self.eps)).log()\n",
    "        \n",
    "        logit_alpha = logit_pi + inter_z.view(N,k,1)\n",
    "        alpha = logit_alpha.sigmoid()\n",
    "        \n",
    "        z1 = (logit_alpha + logit_G1)/self.temperature[:k,:].view(1,k,1)\n",
    "        z = z1.sigmoid()\n",
    "        \n",
    "        code = z_gauss*z.mean(dim=-1).view(N,k) # N x k   : this will the output of the network.....\n",
    "        \n",
    "        if(not self.store_KL):    \n",
    "            return code \n",
    "        \n",
    "        ################################################################################\n",
    "        \n",
    "        #        This layer code is written by \"Abhishek Kumar\" github : scakc         #\n",
    "        \n",
    "        ################################################################################\n",
    "        ####### Calculation of KL divergence for this layer with priors.....\n",
    "        curr_K = self.curr_out\n",
    "        eps = self.eps\n",
    "        \n",
    "        \n",
    "        # KL Gauss\n",
    "        KL_gauss = (-0.5*(1 + log_var - mu.pow(2) - log_var.exp()).view(N,curr_K).mean(dim = 0).view(1,-1))\n",
    "        KL_gauss[KL_gauss != KL_gauss] = 0\n",
    "        KL_gauss = KL_gauss.view(curr_K, 1)# curr_K x 1\n",
    "        \n",
    "        \n",
    "        # KL Kumaraswamy \n",
    "        a = self.softplus(self.beta_a[:curr_K,:]).view(curr_K,1)\n",
    "        b = self.softplus(self.beta_b[:curr_K,:]).view(curr_K,1)\n",
    "        euler_constant = -torch.digamma(torch.tensor(1.0))\n",
    "        KL_kuma = ((a - self.alpha)/(a))*(-euler_constant -torch.digamma(b) - 1/b)\n",
    "        KL_kuma += (a.log() + b.log()) + torch.log(torch.tensor(BETA(self.alpha,1)))\n",
    "        KL_kuma -= (b - 1)/(b) \n",
    "        KL_kuma[KL_kuma != KL_kuma] = 0 \n",
    "        KL_kuma = KL_kuma.view(curr_K,1)  # curr_K x 1\n",
    "        \n",
    "        \n",
    "        # KL Gumbel \n",
    "        logit_pi = (pi+eps).log() - (1-pi+eps).log()\n",
    "        logit_x  =  (z+eps).log() - (1 -z+eps).log()\n",
    "        logit_gi = (alpha+eps).log() - (1-alpha+eps).log()\n",
    "        \n",
    "        tau = self.temperature[:curr_K,:].view(1,curr_K,1)\n",
    "        tau_prior = self.t_prior\n",
    "        \n",
    "        exp_term_p = logit_pi - logit_x*(tau)\n",
    "        exp_term_q = logit_gi - logit_x*(tau)\n",
    "        log_tau = torch.log(torch.tensor(tau, requires_grad = False))\n",
    "        log_taup = torch.log(torch.tensor(tau_prior, requires_grad = False))\n",
    "        \n",
    "        softplus = torch.nn.Softplus(threshold = 20)\n",
    "        log_pz = log_tau + exp_term_p - 2.0*softplus(exp_term_p)\n",
    "        log_qz = log_tau + exp_term_q - 2.0*softplus(exp_term_q)\n",
    "        KL_gumb = (log_qz - log_pz)\n",
    "        KL_gumb[KL_gumb != KL_gumb] = 0\n",
    "        KL_gumb = KL_gumb.mean(dim =-1).mean(dim = 0).view(curr_K,1) # curr_K x 1\n",
    "        \n",
    "        self.KLs = [KL_gauss, KL_kuma, KL_gumb]\n",
    "        self.code = code\n",
    "        \n",
    "        \n",
    "        return code\n",
    "    \n",
    "    \n",
    "    def update_layer(self, in_features, out_features):\n",
    "        \n",
    "        if(out_features > self.out_features):\n",
    "            k = out_features - self.out_features\n",
    "            with torch.no_grad():\n",
    "                self.beta_a = nn.Parameter(torch.cat((self.beta_a, torch.rand(k,1)*self.eps + self.softplus(self.alpha)), 0))\n",
    "                self.beta_b = nn.Parameter(torch.cat((self.beta_b, torch.rand(k,1)*self.eps + self.softplus(self.beta)), 0))\n",
    "\n",
    "                self.phi = nn.Parameter(torch.cat((self.phi, torch.randn(self.in_features,k)*self.eps), 1))\n",
    "                self.weight = nn.Parameter(torch.cat((self.weight, torch.randn(k, self.in_features)), 0)*self.eps)\n",
    "                self.weight_std = nn.Parameter(torch.cat((self.weight_std, torch.randn(k, self.in_features)), 0)*self.eps)\n",
    "                self.bias = nn.Parameter(torch.cat((self.bias, torch.randn(k, 1)), 0)*self.eps)\n",
    "                self.bias_std = nn.Parameter(torch.cat((self.bias_std, torch.randn(k, 1)), 0)*self.eps)\n",
    "                self.bias_phi = nn.Parameter(torch.cat((self.bias_phi, torch.randn(k, 1)), 0)*self.eps)\n",
    "                self.rhos = torch.cat((self.rhos, torch.zeros(k,1) + 0.5), 0)\n",
    "                self.temperature = torch.cat((self.temperature, torch.zeros(k,1) + 10.0), 0)\n",
    "                \n",
    "                self.out_features = out_features\n",
    "                \n",
    "        self.curr_out = out_features\n",
    "            \n",
    "        if(in_features > self.in_features):\n",
    "            k = in_features - self.in_features\n",
    "            with torch.no_grad():\n",
    "                self.phi = nn.Parameter(torch.cat((self.phi, torch.randn(k,self.out_features)*self.eps*0), 0))\n",
    "                self.weight = nn.Parameter(torch.cat((self.weight, torch.randn(self.out_features, k)), 1)*self.eps*0)\n",
    "                self.weight_std = nn.Parameter(torch.cat((self.weight_std, torch.randn(self.out_features, k)), 1)*self.eps*0)\n",
    "\n",
    "                self.in_features = in_features\n",
    "    \n",
    "        self.curr_in = in_features\n",
    "    \n",
    "    def constraint_proj(self):\n",
    "        with torch.no_grad():\n",
    "            self.beta_a[self.beta_a < 0.1] = 0.1\n",
    "            self.beta_b[self.beta_b < 0.1] = 0.1\n",
    "            self.rhos[self.rhos < 10e-6] = 10e-6\n",
    "            self.rhos[self.rhos > 1 - 10e-8] = 1 - 10e-8\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class capBatchNorm2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(capBatchNorm2d, self).__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x = input # NxD\n",
    "        eps = torch.tensor(10e-6)\n",
    "        N, D = x.shape\n",
    "        mean = x.mean(dim = 0).view(1,D)\n",
    "        std = (x-mean + eps).pow(2).sum(dim = 0).div(N).pow(0.5)\n",
    "        \n",
    "        x_norm = (x-mean)/(torch.max(std,torch.tensor(1.0)) + eps)\n",
    "        x_norm += torch.rand(x_norm.shape)*0.001\n",
    "        \n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIC(nn.Module):\n",
    "    def __init__(self, latent_variable_dim , rholr = 10e-2, lr = 0.01):\n",
    "        super(APIC, self).__init__()\n",
    "       \n",
    "        # Encoder\n",
    "        self.D = 784\n",
    "        self.h_dim = 5\n",
    "        self.sl0 = stochasticIBP(self.D,self.h_dim,True, fix = [True, False], alpha = 4.0)\n",
    "        self.sl1 = stochasticIBP(self.h_dim, latent_variable_dim)\n",
    "        self.sl2 = stochasticIBP(latent_variable_dim, self.h_dim, alpha = 4.0)\n",
    "        self.sl3 = stochasticIBP(self.h_dim, self.D,True, fix = [False, True])\n",
    "        \n",
    "        self.bn = capBatchNorm2d()\n",
    "        \n",
    "        self.sls = [self.sl0, self.sl1, self.sl2]\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.lr = lr\n",
    "        self.rholr = rholr\n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, self.D)\n",
    "        x = F.relu(self.sl0(x))\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.sl1(x))\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.sl2(x))\n",
    "        x = self.bn(x)\n",
    "        x = self.sl3(x).sigmoid()\n",
    "        return x\n",
    "    \n",
    "    def train_layer(self, layer, images, sample_max = 10):\n",
    "        # if its not a stochastic layer with fixout as False then normal flow else\n",
    "        \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        curr_K = layer.curr_out\n",
    "        layer.rhos[0] = 1\n",
    "        with torch.no_grad():\n",
    "            rhos = list(layer.rhos)[:curr_K+1]\n",
    "            L = len(rhos)\n",
    "            samples = []\n",
    "            for i in range(sample_max):\n",
    "                k = 1\n",
    "                while(True):\n",
    "                    u = np.random.uniform()\n",
    "                    if(u > rhos[k]):\n",
    "                        samples.append(k)\n",
    "                        break\n",
    "                    k += 1\n",
    "                    if(k > L-1):\n",
    "                        break\n",
    "                        rhos.append(0.5)\n",
    "                        L = len(rhos)\n",
    "                        \n",
    "        samples.sort()\n",
    "        new_value = int(np.max(samples[-5:]))\n",
    "        layer.update_layer(layer.in_features, new_value)\n",
    "        self.dynamize_Adam()\n",
    "        \n",
    "        \n",
    "        curr_K = layer.curr_out\n",
    "        with torch.no_grad():\n",
    "            while(len(rhos)<curr_K+1):\n",
    "                rhos.append(0.5)\n",
    "            omrho = 1 - np.array(rhos)[:curr_K+1]\n",
    "            weight = omrho*0\n",
    "            for i in range(len(samples)):\n",
    "                val = samples[i]+1\n",
    "                weight[:val] += omrho[:val]\n",
    "            weight/=len(samples)\n",
    "            weightf = torch.tensor(weight).view(1,-1).float()\n",
    "        \n",
    "        model.optimizer.zero_grad()\n",
    "        one_minus_rho = weightf[:,:curr_K+1]\n",
    "        \n",
    "        l = torch.zeros(curr_K+1,1)\n",
    "#         print(l.shape, curr_K, layer.curr_out)\n",
    "        one_minus_rho = one_minus_rho.view(-1,1)\n",
    "        weight = torch.zeros_like(one_minus_rho)\n",
    "        weight[curr_K] = one_minus_rho[curr_K]\n",
    "        for i in range(1,curr_K+1):\n",
    "            weight[curr_K-i] = weight[curr_K-i+1] + one_minus_rho[curr_K - i]\n",
    "        \n",
    "        N = images.shape[0]\n",
    "        global_multiplier = 0.001\n",
    "        \n",
    "        \n",
    "#         print(layer.curr_out)\n",
    "        layer.store_KL = True\n",
    "        layer.code = None\n",
    "        _ = self.forward(images)\n",
    "        KL_gauss, KL_kuma, KL_gumb = layer.KLs\n",
    "#         print(KL_gauss.shape, KL_kuma.shape, KL_gumb.shape, l.shape, layer.curr_out)\n",
    "        layer.KLs = None\n",
    "        layer.store_KL = False\n",
    "        \n",
    "        l[1:,:] += KL_gauss + KL_kuma + KL_gumb\n",
    "        \n",
    "        \n",
    "        #likelihood\n",
    "        lik_loss = 0\n",
    "        eps = layer.eps\n",
    "        softplus = torch.nn.Softplus(threshold = 20)\n",
    "        for i in range(1,curr_K+1):\n",
    "            sigma2X = 0.01\n",
    "            layer.curr_out = i\n",
    "            recon_image = model(images)\n",
    "            logity = ((recon_image + eps).log() - (1 - recon_image + eps).log()).view(-1,model.D)\n",
    "            \n",
    "            Lik = images.view(-1,model.D)*logity - softplus(logity)\n",
    "            Lik = torch.sum(Lik)/(N)\n",
    "\n",
    "            lik_loss += Lik*one_minus_rho[i]\n",
    "            l[i,:] -= Lik\n",
    "        \n",
    "        layer.code = None\n",
    "        weight = weight[1:]\n",
    "        v0 = - lik_loss\n",
    "        v1 = (KL_gauss*weight.view(-1,1)).sum()\n",
    "        v2 = (KL_gumb*weight.view(-1,1)).sum()\n",
    "        v3 = (KL_kuma*global_multiplier*weight.view(-1,1)).sum()\n",
    "        \n",
    "        l_final_params = v0+v1+v2+v3\n",
    "        l_final_params.backward()\n",
    "        self.optimizer.step()\n",
    "        layer.constraint_proj()\n",
    "        layer.temperature /= 1.01\n",
    "        layer.temperature[layer.temperature < layer.t_prior] = layer.t_prior\n",
    "        \n",
    "        rgg = torch.zeros_like(layer.rhos)\n",
    "        for ck in samples:\n",
    "            ckp1 = ck + 1\n",
    "            rho_grads = [torch.tensor([0])]\n",
    "            ckp1 = len(rhos)\n",
    "            rho_rr = list(layer.rhos)[:ckp1]\n",
    "            l_rr = l[1:ckp1]\n",
    "\n",
    "\n",
    "            for k in range(1,ckp1):\n",
    "\n",
    "                grad = 0.0\n",
    "\n",
    "                if(k >= l.shape[0]):\n",
    "                    rho_grads.append(rho_grads[0])\n",
    "                    continue\n",
    "                else:\n",
    "                    for i in range(k-1, l_rr.shape[0]):\n",
    "                        wi = 0\n",
    "                        if(k-1 == i):\n",
    "                            wi = 1/(rho_rr[k-1] - 1)\n",
    "                        else:\n",
    "                            wi = 1/rho_rr[k-1]\n",
    "\n",
    "                        grad += (1-rho_rr[i+1])*wi*l_rr[i]\n",
    "\n",
    "                rho_grads.append(grad)\n",
    "            rho_grads = torch.tensor(rho_grads).view(-1,1)\n",
    "            rgg[:ckp1] += rho_grads\n",
    "\n",
    "        rgg/=len(samples)\n",
    "        rho_grads = rgg[:curr_K+1].clamp(-1000,1000)\n",
    "\n",
    "        rho_grads[rho_grads != rho_grads] = 0.0\n",
    "        rho_logit = ((layer.rhos).log() - (1 - layer.rhos).log())[:curr_K+1]\n",
    "        sig_rho = rho_logit.sigmoid()\n",
    "        \n",
    "\n",
    "        rho_logit[:curr_K+1,:] = rho_logit[:curr_K+1,:] - self.rholr*(sig_rho*(1-sig_rho)*rho_grads.view(-1,1))\n",
    "        with torch.no_grad():\n",
    "            layer.rhos[:curr_K+1,:] = (rho_logit).sigmoid()\n",
    "            layer.rhos[layer.rhos<0.1] = layer.rhos[layer.rhos<0.1]*0.1 + 0.1\n",
    "        return l_final_params\n",
    "        \n",
    "    \n",
    "    def dynamize_Adam(self, reset = False, amsgrad = True):\n",
    "        with torch.no_grad():\n",
    "            if(reset or self.optimizer == None):\n",
    "                self.optimizer = AdaRadM(self.parameters(), self.lr, amsgrad = amsgrad)\n",
    "                self.optimizer.step()\n",
    "            else:\n",
    "                optim = self.optimizer\n",
    "                newoptim = AdaRadM(self.parameters(), self.lr)\n",
    "\n",
    "                for i in range(len(optim.param_groups)):\n",
    "                    group_old = optim.param_groups[i]\n",
    "                    group_new = newoptim.param_groups[i]\n",
    "\n",
    "                    for j in range(len(group_old['params'])):\n",
    "                        params_old = group_old['params'][j]\n",
    "                        params_new = group_new['params'][j]\n",
    "\n",
    "                        amsgrad = group_old['amsgrad']\n",
    "                        newoptim.param_groups[i]['amsgrad'] = amsgrad\n",
    "\n",
    "\n",
    "                        state_old = optim.state[params_old]\n",
    "                        state_new = newoptim.state[params_new]\n",
    "\n",
    "                        state_new['step'] = torch.zeros_like(params_new.data)\n",
    "\n",
    "                        state_new['exp_avg'] = torch.zeros_like(params_new.data)\n",
    "                        state_new['exp_avg_sq'] = torch.zeros_like(params_new.data)\n",
    "\n",
    "\n",
    "\n",
    "                        exp_avg = state_new['exp_avg']\n",
    "                        exp_avg_sq = state_new['exp_avg_sq']\n",
    "                        max_exp_avg_sq = None\n",
    "                        if(amsgrad):\n",
    "                            state_new['max_exp_avg_sq'] = torch.zeros_like(params_new.data)\n",
    "                            max_exp_avg_sq = state_new['max_exp_avg_sq']\n",
    "                            \n",
    "                        if(len(state_old) == 0):\n",
    "                            pass\n",
    "                        else:\n",
    "                            if(len(state_old['exp_avg'].shape)==2):\n",
    "                                no,do = state_old['exp_avg'].shape\n",
    "                                exp_avg[:no,:do] = state_old['exp_avg']\n",
    "                                exp_avg_sq[:no,:do] = state_old['exp_avg_sq']\n",
    "                                if(max_exp_avg_sq is not None):\n",
    "                                    max_exp_avg_sq[:no,:do] = state_old['max_exp_avg_sq']\n",
    "                                state_new['step'][:no,:do] = state_old['step']\n",
    "\n",
    "                            elif(len(state_old['exp_avg'].shape)==1):\n",
    "                                no = state_old['exp_avg'].shape[0]\n",
    "                                exp_avg[:no] = state_old['exp_avg']\n",
    "                                exp_avg_sq[:no] = state_old['exp_avg_sq']\n",
    "                                if(max_exp_avg_sq is not None):\n",
    "                                    max_exp_avg_sq[:no] = state_old['max_exp_avg_sq']\n",
    "                                state_new['step'][:no] = state_old['step']\n",
    "\n",
    "                            else:\n",
    "                                assert 1 == 2 ,'error in dynamic adam'\n",
    "\n",
    "                        state_new['exp_avg'] = exp_avg\n",
    "                        state_new['exp_avg_sq'] = exp_avg_sq\n",
    "\n",
    "                        newoptim.state[params_new] = state_new\n",
    "                    \n",
    "                self.optimizer = newoptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "model = APIC(4, lr = 0.0000001, rholr = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dynamize_Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 7.1274e-10],\n",
      "        [-1.7825e-09],\n",
      "        [-1.5268e-09],\n",
      "        [-1.7346e-10],\n",
      "        [ 6.3545e-10]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/abhi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/abhi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(0.9934)\n",
      "Angle tensor(0.9934)\n",
      "Angle tensor(0.9934)\n",
      "Angle tensor(0.9934)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(0.9996)\n",
      "Angle tensor(0.9996)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9995)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9994)\n",
      "Angle tensor(0.9993)\n",
      "Angle tensor(0.9993)\n",
      "Angle tensor(0.9993)\n",
      "Angle tensor(0.9993)\n",
      "Angle tensor(0.9989)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9983)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9979)\n",
      "Angle tensor(0.9950)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9869)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9840)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9839)\n",
      "Angle tensor(0.9834)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9831)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9819)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(0.9800)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "Angle tensor(1.)\n",
      "No. node in  0 th layer is  3 \n",
      "layer_rhos  [0.9999999, 0.500625, 0.5, 0.5003396, 0.5, 0.5]\n",
      "Epoch 1,  Loss, [4.379956], batch 0\n",
      "Parameter containing:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: must be >= 0 and <= 1 at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/TH/THRandom.cpp:320",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-5d39ba433c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_phi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#             assert 1==2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0ml\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No. node in \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"th layer is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nlayer_rhos \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {},  Loss, {}, batch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-1476d9811007>\u001b[0m in \u001b[0;36mtrain_layer\u001b[0;34m(self, layer, images, sample_max)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_KL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mKL_gauss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKL_kuma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKL_gumb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#         print(KL_gauss.shape, KL_kuma.shape, KL_gumb.shape, l.shape, layer.curr_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-1476d9811007>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msl2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msl3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-024254864161>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, k, sample_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mlogit_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/bernoulli.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: must be >= 0 and <= 1 at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/TH/THRandom.cpp:320"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(500):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        \n",
    "        l = 0\n",
    "        for k in range(len(model.sls)):\n",
    "            layer = model.sls[::-1][k]\n",
    "            print(layer.bias_phi)\n",
    "#             assert 1==2\n",
    "            l += model.train_layer(layer, images, sample_max = 50)\n",
    "            print(\"No. node in \", k, \"th layer is \", layer.curr_out, \"\\nlayer_rhos \", list(layer.rhos.view(-1).numpy()))\n",
    "            print(\"Epoch {},  Loss, {}, batch {}\".format(epoch+1, l.clone().detach().numpy()/(BATCH_SIZE*(k+1)), i))\n",
    "#             clr(wait = True)\n",
    "            model.dynamize_Adam()\n",
    "            \n",
    "        \n",
    "        \n",
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: must be >= 0 and <= 1 at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/TH/THRandom.cpp:320",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-802-b96e70ee2404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-798-0e5b728b9b28>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msl2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msl3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-796-024254864161>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, k, sample_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mlogit_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/bernoulli.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: must be >= 0 and <= 1 at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/TH/THRandom.cpp:320"
     ]
    }
   ],
   "source": [
    "show_images(model(images).view(-1,1,28,28).detach())\n",
    "plt.show()\n",
    "show_images(images.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 7.5733e-11, -1.2577e-10, -9.4350e-10, -8.8447e-11],\n",
       "        [ 1.0840e-09, -5.4320e-10,  5.5608e-10, -6.9973e-10],\n",
       "        [-5.2577e-10,  2.7499e-10,  6.9355e-10,  6.2800e-10],\n",
       "        [ 4.1697e-10,  9.7640e-10, -6.3254e-10,  2.0852e-09],\n",
       "        [        nan,         nan,         nan,         nan]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.4463e-09],\n",
       "        [1.8346e-09],\n",
       "        [2.7423e-09],\n",
       "        [       nan]], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
